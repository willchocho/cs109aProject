{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A\n",
    "# Final Project- EDA Milestone 3\n",
    "\n",
    "# Will Cho, Haruka Uchida, Jessica Zhao\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#import pydotplus\n",
    "#import io\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from matplotlib import colors\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################\n",
    "### Read in Data ########\n",
    "#########################\n",
    "\n",
    "# if want to use ID number as the index, index_col = 'RID'\n",
    "df = pd.read_csv('Data_Database/ADNIMERGE.csv')\n",
    "\n",
    "#################################\n",
    "### Get all Column Names ########\n",
    "#################################\n",
    "\n",
    "all_variables_first = list(df.columns.values)\n",
    "\n",
    "#################################\n",
    "### Make Dummy Variables ########\n",
    "#################################\n",
    "\n",
    "# there are probably more categoricals need to add to this array\n",
    "categorical_col = ['DX', 'PTGENDER']\n",
    "df = pd.get_dummies(df, columns=categorical_col)\n",
    "# make list of all variables, including dummy variables\n",
    "all_variables_dum = list(df.columns.values)\n",
    "len(all_variables_dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>SITE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <th>APOE4</th>\n",
       "      <th>FDG</th>\n",
       "      <th>PIB</th>\n",
       "      <th>AV45</th>\n",
       "      <th>CDRSB</th>\n",
       "      <th>ADAS11</th>\n",
       "      <th>...</th>\n",
       "      <th>AV45_bl</th>\n",
       "      <th>Years_bl</th>\n",
       "      <th>Month_bl</th>\n",
       "      <th>Month</th>\n",
       "      <th>M</th>\n",
       "      <th>DX_CN</th>\n",
       "      <th>DX_Dementia</th>\n",
       "      <th>DX_MCI</th>\n",
       "      <th>PTGENDER_Female</th>\n",
       "      <th>PTGENDER_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13017.000000</td>\n",
       "      <td>13017.000000</td>\n",
       "      <td>13017.000000</td>\n",
       "      <td>13017.000000</td>\n",
       "      <td>12958.000000</td>\n",
       "      <td>3353.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>2161.000000</td>\n",
       "      <td>9016.000000</td>\n",
       "      <td>8959.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5969.000000</td>\n",
       "      <td>13017.000000</td>\n",
       "      <td>13017.000000</td>\n",
       "      <td>13017.000000</td>\n",
       "      <td>13017.000000</td>\n",
       "      <td>13017.000000</td>\n",
       "      <td>13017.000000</td>\n",
       "      <td>13017.000000</td>\n",
       "      <td>13017.000000</td>\n",
       "      <td>13017.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2285.200584</td>\n",
       "      <td>73.892064</td>\n",
       "      <td>73.767220</td>\n",
       "      <td>15.994930</td>\n",
       "      <td>0.535654</td>\n",
       "      <td>1.208225</td>\n",
       "      <td>1.783161</td>\n",
       "      <td>1.195504</td>\n",
       "      <td>2.163598</td>\n",
       "      <td>11.398507</td>\n",
       "      <td>...</td>\n",
       "      <td>1.200700</td>\n",
       "      <td>2.235266</td>\n",
       "      <td>26.768221</td>\n",
       "      <td>26.688868</td>\n",
       "      <td>26.498041</td>\n",
       "      <td>0.213106</td>\n",
       "      <td>0.162096</td>\n",
       "      <td>0.312284</td>\n",
       "      <td>0.436199</td>\n",
       "      <td>0.563801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1871.013213</td>\n",
       "      <td>110.533877</td>\n",
       "      <td>6.979685</td>\n",
       "      <td>2.824862</td>\n",
       "      <td>0.655480</td>\n",
       "      <td>0.160972</td>\n",
       "      <td>0.422511</td>\n",
       "      <td>0.227999</td>\n",
       "      <td>2.805879</td>\n",
       "      <td>8.616859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221538</td>\n",
       "      <td>2.225071</td>\n",
       "      <td>26.646135</td>\n",
       "      <td>26.609785</td>\n",
       "      <td>26.398907</td>\n",
       "      <td>0.409518</td>\n",
       "      <td>0.368553</td>\n",
       "      <td>0.463443</td>\n",
       "      <td>0.495932</td>\n",
       "      <td>0.495932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>54.400000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.636804</td>\n",
       "      <td>1.095000</td>\n",
       "      <td>0.814555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>631.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>69.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.109730</td>\n",
       "      <td>1.361250</td>\n",
       "      <td>1.010140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.330000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.018950</td>\n",
       "      <td>0.498289</td>\n",
       "      <td>5.967210</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1301.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>73.700000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.219870</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>1.114670</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.120310</td>\n",
       "      <td>1.524980</td>\n",
       "      <td>18.262300</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4353.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>78.600000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.314320</td>\n",
       "      <td>2.127500</td>\n",
       "      <td>1.364980</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.374980</td>\n",
       "      <td>3.091030</td>\n",
       "      <td>37.016400</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6094.000000</td>\n",
       "      <td>941.000000</td>\n",
       "      <td>91.400000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.753320</td>\n",
       "      <td>2.927500</td>\n",
       "      <td>2.669210</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.025560</td>\n",
       "      <td>11.036300</td>\n",
       "      <td>132.164000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                RID          SITE           AGE      PTEDUCAT         APOE4  \\\n",
       "count  13017.000000  13017.000000  13017.000000  13017.000000  12958.000000   \n",
       "mean    2285.200584     73.892064     73.767220     15.994930      0.535654   \n",
       "std     1871.013213    110.533877      6.979685      2.824862      0.655480   \n",
       "min        2.000000      2.000000     54.400000      4.000000      0.000000   \n",
       "25%      631.000000     21.000000     69.500000     14.000000      0.000000   \n",
       "50%     1301.000000     41.000000     73.700000     16.000000      0.000000   \n",
       "75%     4353.000000    116.000000     78.600000     18.000000      1.000000   \n",
       "max     6094.000000    941.000000     91.400000     20.000000      2.000000   \n",
       "\n",
       "               FDG         PIB         AV45        CDRSB       ADAS11  \\\n",
       "count  3353.000000  223.000000  2161.000000  9016.000000  8959.000000   \n",
       "mean      1.208225    1.783161     1.195504     2.163598    11.398507   \n",
       "std       0.160972    0.422511     0.227999     2.805879     8.616859   \n",
       "min       0.636804    1.095000     0.814555     0.000000     0.000000   \n",
       "25%       1.109730    1.361250     1.010140     0.000000     5.330000   \n",
       "50%       1.219870    1.850000     1.114670     1.000000     9.000000   \n",
       "75%       1.314320    2.127500     1.364980     3.000000    15.000000   \n",
       "max       1.753320    2.927500     2.669210    18.000000    70.000000   \n",
       "\n",
       "           ...            AV45_bl      Years_bl      Month_bl         Month  \\\n",
       "count      ...        5969.000000  13017.000000  13017.000000  13017.000000   \n",
       "mean       ...           1.200700      2.235266     26.768221     26.688868   \n",
       "std        ...           0.221538      2.225071     26.646135     26.609785   \n",
       "min        ...           0.838537      0.000000      0.000000      0.000000   \n",
       "25%        ...           1.018950      0.498289      5.967210      6.000000   \n",
       "50%        ...           1.120310      1.524980     18.262300     18.000000   \n",
       "75%        ...           1.374980      3.091030     37.016400     36.000000   \n",
       "max        ...           2.025560     11.036300    132.164000    132.000000   \n",
       "\n",
       "                  M         DX_CN   DX_Dementia        DX_MCI  \\\n",
       "count  13017.000000  13017.000000  13017.000000  13017.000000   \n",
       "mean      26.498041      0.213106      0.162096      0.312284   \n",
       "std       26.398907      0.409518      0.368553      0.463443   \n",
       "min        0.000000      0.000000      0.000000      0.000000   \n",
       "25%        6.000000      0.000000      0.000000      0.000000   \n",
       "50%       18.000000      0.000000      0.000000      0.000000   \n",
       "75%       36.000000      0.000000      0.000000      1.000000   \n",
       "max      126.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       PTGENDER_Female  PTGENDER_Male  \n",
       "count     13017.000000   13017.000000  \n",
       "mean          0.436199       0.563801  \n",
       "std           0.495932       0.495932  \n",
       "min           0.000000       0.000000  \n",
       "25%           0.000000       0.000000  \n",
       "50%           0.000000       1.000000  \n",
       "75%           1.000000       1.000000  \n",
       "max           1.000000       1.000000  \n",
       "\n",
       "[8 rows x 82 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################\n",
    "### Basic Summary Stats #########\n",
    "#################################\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now, we will fill missing values with 0s\n",
    "# replace NaNs with 0s\n",
    "df.fillna(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "### Make Var for whether have Alz or not ####\n",
    "#############################################\n",
    "\n",
    "#currently the diagnosis variables = 1 if has any form of dementia\n",
    "#want to make dummy variable for if they report having alzheimers (not any other type)\n",
    "\n",
    "alz_at_moment = []\n",
    "# for each observation in the dataset\n",
    "for observation in range(len(df.index)):\n",
    "    if df['DX_bl'][observation] == 'AD':\n",
    "        alz_at_moment.append(1)\n",
    "    else:\n",
    "        alz_at_moment.append(0)\n",
    "df['alz_at_moment'] = alz_at_moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "## Make Predictor Variable ##\n",
    "#############################\n",
    "\n",
    "# since data is longitudinal, will make a \"did they ever get alzheimers\" variable\n",
    "# make a list of the alzheimer dummy values for that patient\n",
    "#[visit for visit in list(df['DX_Dementia'].groupby(df['PTID']))[1]]\n",
    "got_alz = []\n",
    "#for every unique patient\n",
    "for name, group in df.groupby('PTID'):\n",
    "    # list of patient's alzheimer dummy values (since longitudinal)\n",
    "    unique_patient_alz = group['alz_at_moment'].values\n",
    "    # get 'max' of alzheimers dummy\n",
    "    # therefore those with max = 1 have reported getting alzheimers in their life\n",
    "    # those with max = 0 never got alzheimers\n",
    "    got_alz = got_alz + [max(unique_patient_alz)] * len(unique_patient_alz)\n",
    "\n",
    "#add it to the dataframe\n",
    "df['got_alz'] = got_alz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.000000\n",
       "1        0.000000\n",
       "2        0.498289\n",
       "3        0.999316\n",
       "4        1.998630\n",
       "5        0.000000\n",
       "6        0.479124\n",
       "7        1.015740\n",
       "8        1.511290\n",
       "9        3.028060\n",
       "10       0.000000\n",
       "11       0.501027\n",
       "12       0.993840\n",
       "13       1.998630\n",
       "14       3.008900\n",
       "15       0.000000\n",
       "16       0.503765\n",
       "17       0.974675\n",
       "18       1.456540\n",
       "19       3.025330\n",
       "20       0.000000\n",
       "21       0.517454\n",
       "22       0.000000\n",
       "23       0.498289\n",
       "24       0.000000\n",
       "25       0.492813\n",
       "26       0.996578\n",
       "27       1.990420\n",
       "28       0.000000\n",
       "29       0.490075\n",
       "           ...   \n",
       "12987    0.000000\n",
       "12988    0.000000\n",
       "12989    0.618754\n",
       "12990    2.193020\n",
       "12991    0.000000\n",
       "12992    0.465435\n",
       "12993    2.967830\n",
       "12994    0.000000\n",
       "12995    2.384670\n",
       "12996    0.000000\n",
       "12997    2.031490\n",
       "12998    0.000000\n",
       "12999    0.465435\n",
       "13000    1.995890\n",
       "13001    0.000000\n",
       "13002    0.517454\n",
       "13003    2.012320\n",
       "13004    0.000000\n",
       "13005    0.498289\n",
       "13006    2.017800\n",
       "13007    0.000000\n",
       "13008    0.484600\n",
       "13009    2.017800\n",
       "13010    0.000000\n",
       "13011    2.480490\n",
       "13012    0.000000\n",
       "13013    0.177960\n",
       "13014    1.990420\n",
       "13015    0.000000\n",
       "13016    1.995890\n",
       "Name: Years_bl, Length: 13017, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Years_bl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "### Make Age at Exam Date Variable ###\n",
    "######################################\n",
    "\n",
    "age_at_examdate = []\n",
    "# for each observation in the dataset\n",
    "for observation in range(len(df.index)):\n",
    "    # age at exam date is 'age at base line' + 'years from baseline'\n",
    "    age_at_examdate.append(df['AGE'][observation] + df['Years_bl'][observation])\n",
    "df['age_at_examdate'] = age_at_examdate\n",
    "\n",
    "#Now we can get rid of the 'years since baseline' variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "### Make Training and test data frames ####\n",
    "###########################################\n",
    "\n",
    "np.random.seed(9001)\n",
    "msk = np.random.rand(len(df)) < 0.5\n",
    "df_train = df[msk]\n",
    "df_test = df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87689249120660651"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# proportion of observations in training set that do not have alzheimers\n",
    "# not alz / (alz + not alz)\n",
    "# use frequency table\n",
    "# http://hamelg.blogspot.com/2015/11/python-for-data-analysis-part-19_17.html\n",
    "pd.crosstab(index=df_train['got_alz'], columns=\"count\")['count'][0] / (pd.crosstab(index=df_train['got_alz'], columns=\"count\")['count'][0]+pd.crosstab(index=df_train['got_alz'], columns=\"count\")['count'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "## scaling test set and training set continuous variables ###\n",
    "#############################################################\n",
    "numerical_columns = []\n",
    "for var in numerical_columns:\n",
    "    mean = df_train[var].mean()\n",
    "    std = df_train[var].std()\n",
    "\n",
    "    df_train[var] = (df_train[var] - mean)/std\n",
    "    df_test[var] = (df_test[var] - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "### Prediction Model: Risk of Developing Alzheimers ####\n",
    "########################################################\n",
    "\n",
    "# Build a classification model that classifies patients for eventually developing alz or not\n",
    "\n",
    "# This is an unbalanced dataset!!!!!!\n",
    "\n",
    "#y_train_risk = df_train['got_alz'].values\n",
    "#X_train_risk = df_train[risk_predictors].values\n",
    "\n",
    "#y_test_risk = df_test['got_alz'].values\n",
    "#X_test_risk = df_test[risk_predictors].values\n",
    "\n",
    "## will try 4 different models and compare to decide which is best:\n",
    "## 1. logistic regression\n",
    "## 2. LDA/QDA\n",
    "## 3. Decision Trees\n",
    "\n",
    "# will not k-NN as it is inefficient for a big number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "### Multinomial Logistic Regression ##\n",
    "######################################\n",
    "\n",
    "#add a penalty to prevent overfitting\n",
    "\n",
    "#logit_model = LogisticRegressionCV(random_state = 123, penalty='l2')\n",
    "#logit_model.fit(X_train_classif, y_train_classif)\n",
    "#predict_Logistic_train = logit_model.predict(X_train_classif)\n",
    "#predict_Logistic_test = logit_model.predict(X_test_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "### Linear Discriminant Analysis #####\n",
    "######################################\n",
    "\n",
    "#lda = da.LinearDiscriminantAnalysis()\n",
    "#lda.fit(X_train_classif, y_train_classif)\n",
    "#predict_LDA_train = lda.predict(X_train_classif)\n",
    "#predict_LDA_test = lda.predict(X_test_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "### Quadratic Discriminant Analysis #####\n",
    "#########################################\n",
    "\n",
    "#qda = QuadraticDiscriminantAnalysis()\n",
    "#qda.fit(X_train_classif, y_train_classif)\n",
    "#predict_QDA_train = qda.predict(X_train_classif)\n",
    "#predict_QDA_test = qda.predict(X_test_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "### Decision Trees Analysis #####\n",
    "#################################\n",
    "#code from midterm solutions\n",
    "\n",
    "#dec_tree = DecisionTreeClassifier()\n",
    "#depths = {'max_depth': list(range(2,15))}\n",
    "#dec_tree_grid = sk.model_selection.GridSearchCV(dec_tree, depths, cv=5, scoring='roc_auc')\n",
    "#dec_tree_grid.fit(X_train_classif, y_train_classif)\n",
    "#best_depth = max([(s,d) for s,d in zip(dec_tree_grid.cv_results_['mean_test_score'],list(range(2,15)))],key=lambda x: x[0])[1]\n",
    "#dec_tree_best = DecisionTreeClassifier(max_depth=best_depth).fit(X_train_classif, y_train_classif)\n",
    "#predict_Tree_train = tree_model.predict(X_train_classif)\n",
    "#predict_Tree_test = tree_model.predict(X_test_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "### Summary Scores of Models #####\n",
    "##################################\n",
    "# on the train and test set\n",
    "\n",
    "# Here we use the ROC AUC score because the data is unbalanced\n",
    "\n",
    "\n",
    "## SHOULD THE PREDICTIONS BE PREDICT_PROBA INSTEAAD OF PREDICT????????\n",
    "\n",
    "print('Logistic Regression model accuracy for training set:', sk.metrics.roc_auc_score(y_train, predict_Logistic_train))\n",
    "print('Logistic Regression model accuracy for test set:', sk.metrics.roc_auc_score(y_test, predict_Logistic_test))\n",
    "\n",
    "print('LDA model accuracy for training set:', sk.metrics.roc_auc_score(y_train, predict_LDA_train))\n",
    "print('LDA model accuracy for test set:', sk.metrics.roc_auc_score(y_test, predict_LDA_test), '\\n')\n",
    "\n",
    "print('QDA model accuracy for training set:', sk.metrics.roc_auc_score(y_train, predict_QDA_train))\n",
    "print('QDA model accuracy for test set:', sk.metrics.roc_auc_score(y_test, predict_QDA_test), '\\n')\n",
    "\n",
    "print('Decision Tree model (depth ' + str(best_depth) + ') accuracy for training set:', sk.metrics.roc_auc_score(y_train, predict_Tree_train))\n",
    "print('Decision Tree model (depth ' + str(best_depth) + ') accuracy for test set:', sk.metrics.roc_auc_score(y_test, predict_Tree_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
